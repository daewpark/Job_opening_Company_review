{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 잡플래닛 기업 리뷰 크롤링\n",
    "- 크롤링 과정\n",
    "    1. 회사 이름 검색\n",
    "    2. 리뷰 link 얻기\n",
    "    3. 리뷰 내용 얻기\n",
    "        1. review_text : title,strength,weakness,want (로그인 필요 -> requests.session 사용)\n",
    "        2. stats : request 다시\n",
    "        3. review_num, person, company_name : request 다시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'job_search', using template directory '/Users/juhyunson/opt/anaconda3/lib/python3.7/site-packages/scrapy/templates/project', created in:\r\n",
      "    /Users/juhyunson/Documents/dss0/Programming/crawling 프로젝트/job_search\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd job_search\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "!rm -rf job_search\n",
    "!scrapy startproject job_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# -*- coding: utf-8 -*-\r\n",
      "\r\n",
      "# Define here the models for your scraped items\r\n",
      "#\r\n",
      "# See documentation in:\r\n",
      "# https://docs.scrapy.org/en/latest/topics/items.html\r\n",
      "\r\n",
      "import scrapy\r\n",
      "\r\n",
      "\r\n",
      "class JobSearchItem(scrapy.Item):\r\n",
      "    # define the fields for your item here like:\r\n",
      "    # name = scrapy.Field()\r\n",
      "    pass\r\n"
     ]
    }
   ],
   "source": [
    "!cat job_search/job_search/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job_search/job_search/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile job_search/job_search/items.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class JobSearchItem(scrapy.Item):\n",
    "    link=scrapy.Field()\n",
    "    company_id=scrapy.Field()\n",
    "    company_name=scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    strength = scrapy.Field()\n",
    "    weakness = scrapy.Field()\n",
    "    want=scrapy.Field()\n",
    "    stats = scrapy.Field()    \n",
    "    review_num=scrapy.Field()\n",
    "    person=scrapy.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job_search/job_search/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile job_search/job_search/spiders/spider.py\n",
    "import scrapy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy.http import TextResponse\n",
    "from job_search.items import JobSearchItem\n",
    "import configparser\n",
    "\n",
    "\n",
    "class Spider(scrapy.Spider):\n",
    "    name=\"JobSearch\"\n",
    "    \n",
    "    def __init__(self,company_name='구글코리아',**kwargs):\n",
    "        self.company_name=company_name\n",
    "        self.start_urls='https://www.jobplanet.co.kr/search?query={}&category=&_rs_con=welcome&_rs_act=index&_rs_element=main_search_bar'.format(company_name)\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def start_requests(self):\n",
    "        url=self.start_urls\n",
    "        yield scrapy.Request(url,callback=self.parse)\n",
    "        \n",
    "    def parse(self,response):\n",
    "        item=JobSearchItem()\n",
    "        #로그인 데이터 불러오기\n",
    "        config = configparser.ConfigParser()\n",
    "        config.read('login.ini')\n",
    "        link_first=response.xpath('//*[@id=\"mainContents\"]/div[1]/div/div[2]/div[1]/div/a/@href').extract()[0]\n",
    "        link_first=response.urljoin(link_first)\n",
    "        item['link']=link_first\n",
    "        \n",
    "        #모든 page의 reveiw_text,person 크롤링\n",
    "        page=1\n",
    "        titles_ls,strength_ls,weakness_ls,wants_ls,person_ls=[],[],[],[],[]\n",
    "        while True:\n",
    "            link=link_first.split('info/')[0]+'reviews/'+self.company_name+'?page={}'.format(page)\n",
    "        \n",
    "            #link 접속 \n",
    "            login_url = 'https://www.jobplanet.co.kr/users/sign_in'\n",
    "            login_data = {'user': {'email': config.get('section1', 'ID'), 'password': config.get('section1', 'PW'), 'remember_me':'true'}}\n",
    "            session = requests.session()\n",
    "            req = session.post(login_url, json = login_data) \n",
    "            req = session.get(link)\n",
    "            response_s=TextResponse(req.url, body=req.text, encoding='utf-8')\n",
    "            \n",
    "            #review_text\n",
    "            elements=response_s.xpath('//*[@id=\"viewReviewsList\"]/div/div/div/section/div/div[2]/div/div[1]/h2/text()').extract()\n",
    "            titles = [element.replace(\"\\n\", \"\").strip() for element in elements]\n",
    "            titles = \" \".join(titles)\n",
    "            #title의 길이가 0일때까지 page의 수를 증가시킨다.\n",
    "            if len(titles)==0: break\n",
    "            titles_ls.append(titles)\n",
    "            \n",
    "            elements=response_s.xpath('//*[@id=\"viewReviewsList\"]/div/div/div/section/div/div[2]/div/dl/dd[1]/span/text()').extract()\n",
    "            strengths = [element.replace(\"\\n\", \"\").replace(\"\\r\",\"\").strip() for element in elements]\n",
    "            strengths = \" | \".join(strengths)\n",
    "            strength_ls.append(strengths)\n",
    "            \n",
    "            elements=response_s.xpath('//*[@id=\"viewReviewsList\"]/div/div/div/section/div/div[2]/div/dl/dd[2]/span/text()').extract()\n",
    "            weakness = [element.replace(\"\\n\", \"\").replace(\"\\r\",\"\").strip() for element in elements]\n",
    "            weakness = \" | \".join(weakness)\n",
    "            weakness_ls.append(weakness)\n",
    "            \n",
    "            elements=response_s.xpath('//*[@id=\"viewReviewsList\"]/div/div/div/section/div/div[2]/div/dl/dd[3]/span/text()').extract()\n",
    "            wants = [element.replace(\"\\n\", \"\").replace(\"\\r\",\"\").strip() for element in elements]\n",
    "            wants = \" | \".join(wants)\n",
    "            wants_ls.append(wants)\n",
    "            \n",
    "            #person\n",
    "            person=response_s.xpath('//*[@id=\"viewReviewsList\"]/div/div/div/section/div/div[1]/span/text()').extract()\n",
    "            def divide_list(l, n=7): \n",
    "                # 리스트 l의 길이가 n이면 계속 반복\n",
    "                for i in range(0, len(l), n): \n",
    "                    yield l[i:i + n] \n",
    "            person=list(divide_list(person))[0][::2]\n",
    "            person_ls.append(person)\n",
    "            \n",
    "            page+=1\n",
    "            \n",
    "        item['title']=titles_ls\n",
    "        item['strength']=strength_ls\n",
    "        item['weakness']=weakness_ls\n",
    "        item['want']=wants_ls\n",
    "        item['person'] = person_ls\n",
    "        item['company_name']=self.company_name\n",
    "        company_id = link_first.split(\"companies/\")[1].split('/info')[0]\n",
    "        item['company_id']=company_id\n",
    "\n",
    "        #stats \n",
    "        #request 다시(json으로)\n",
    "        def stats(company_id=company_id):\n",
    "            url = \"https://www.jobplanet.co.kr/api/v3/companies/reviews/premium_rating?company_id={}\".format(company_id)\n",
    "            response2 = requests.get(url)\n",
    "            contents = response2.json()[\"data\"][\"contents\"]\n",
    "            items = contents[0][\"graph_data\"][\"bar_data\"][\"items\"]\n",
    "            stats = {}\n",
    "            for item in items:\n",
    "                stats[item[\"label\"]] = item[\"val\"]\n",
    "            return stats\n",
    "        \n",
    "        item['stats']=stats(company_id)\n",
    "        \n",
    "        #review_num\n",
    "        #request 다시(xpath으로. item을 같은 행에 return 하기 위하여 같은 함수에 request를 적용해줌)\n",
    "        url = link_first\n",
    "        req = requests.get(url)\n",
    "        response3=TextResponse(req.url, body=req.text, encoding='utf-8')\n",
    "        item['review_num']=response3.xpath('//*[@id=\"viewCompaniesMenu\"]/ul/li[2]/a/span/text()').extract()[0].replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "       \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting.py 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Obey robots.txt rules\r\n",
      "ROBOTSTXT_OBEY = True\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 22 job_search/job_search/settings.py | tail -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sed: 1: \"job_search/job_search/s ...\": invalid command code j\r\n"
     ]
    }
   ],
   "source": [
    "!sed -i 's/ROBOTSTXT_OBEY = True/ROBOTSTXT_OBEY = False/' job_search/job_search/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Obey robots.txt rules\r\n",
      "ROBOTSTXT_OBEY = True\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 22 job_search/job_search/settings.py | tail -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mongodb 모듈 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job_search/job_search/mongodb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile job_search/job_search/mongodb.py\n",
    "import pymongo\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "#configFilePath = r'E:ogin.ini'\n",
    "config.read('login.ini')\n",
    "user = config.get('ju', 'ID')\n",
    "pwd = config.get('ju', 'PW')\n",
    "ip = config.get('ju', 'IP')\n",
    "account = f'mongodb://{user}:{pwd}@{ip}'\n",
    "client=pymongo.MongoClient(account, 27017)\n",
    "db=client.jobplanet\n",
    "collection=db.review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pipeline 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job_search/job_search/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile job_search/job_search/pipelines.py\n",
    "from .mongodb import collection\n",
    "\n",
    "class JobSearchPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        data={'company_name':item['company_name'], 'link':item['link'],'company_id':item['company_id'] , \\\n",
    "              'review_num':item['review_num'],'stats':item['stats'],'strength':item['strength'], \\\n",
    "              'title':item['title'], 'want':item['want'], 'weakness':item['weakness'],'person':item['person']}\n",
    "        collection.insert(data)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"ITEM_PIPELINES = {\" >>job_search/job_search/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \" 'job_search.pipelines.JobSearchPipeline':300,\">>job_search/job_search/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"}\">>job_search/job_search/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#HTTPCACHE_IGNORE_HTTP_CODES = []\r\n",
      "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\r\n",
      "ITEM_PIPELINES = {\r\n",
      " 'job_search.pipelines.JobSearchPipeline':300,\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 5 job_search/job_search/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argument 변경하며 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd job_search\n",
    "scrapy crawl JobSearch -o job_planet_ds_review.csv -a company_name='구글코리아'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실행\n",
    "- run.sh 파일을 실행시키면 됌.\n",
    "- 주피터 노트북에서도 매직커맨드와 함께 실행이 가능하지만, 저작권 문제를 피하기 위하여 실행하지 않겠음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
